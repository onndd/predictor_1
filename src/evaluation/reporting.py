#!/usr/bin/env python3
"""
Performance Reporting Utility for JetX Prediction System
"""

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from typing import Dict, List, Any

def explain_test_results(metrics: Dict[str, float]) -> str:
    """
    Test sonuÃ§larÄ±nÄ± TÃ¼rkÃ§e ve anlaÅŸÄ±lÄ±r ÅŸekilde aÃ§Ä±klar.
    
    Args:
        metrics: Test metriklerini iÃ§eren dictionary
        
    Returns:
        AÃ§Ä±klayÄ±cÄ± metin
    """
    explanation = []
    
    # Header
    explanation.append("=" * 80)
    explanation.append("ğŸ¯ TEST SONUÃ‡LARI DETAYLI AÃ‡IKLAMA")
    explanation.append("=" * 80)
    
    # Ana metrikler
    threshold_acc = metrics.get('threshold_accuracy', 0)
    balanced_acc = metrics.get('balanced_accuracy', 0)
    precision = metrics.get('precision', 0)
    recall = metrics.get('recall', 0)
    f1 = metrics.get('f1', 0)
    
    # Confusion matrix deÄŸerleri
    tn = metrics.get('true_negative', 0)
    fp = metrics.get('false_positive', 0)
    fn = metrics.get('false_negative', 0)
    tp = metrics.get('true_positive', 0)
    
    total_tests = tn + fp + fn + tp
    
    explanation.append(f"\nğŸ“Š GENEL PERFORMANS:")
    explanation.append(f"   â€¢ Test edilen Ã¶rnek sayÄ±sÄ±: {int(total_tests)}")
    explanation.append(f"   â€¢ Genel doÄŸruluk: %{threshold_acc*100:.1f}")
    explanation.append(f"   â€¢ F1-Score: %{f1*100:.1f} (Genel kalite gÃ¶stergesi)")
    
    # Model davranÄ±ÅŸ analizi
    explanation.append(f"\nğŸ¤– MODEL DAVRANIÅI ANALÄ°ZÄ°:")
    
    if recall > 0.95:
        explanation.append(f"   âš ï¸  MODEL Ã‡OK AGRESÄ°F! Neredeyse her durumda 'OYNA' diyor (%{recall*100:.1f})")
        explanation.append(f"   ğŸ“ˆ Bu iyi: KazanÃ§ fÄ±rsatlarÄ±nÄ±n Ã§oÄŸunu yakalÄ±yor")
        explanation.append(f"   ğŸ“‰ Bu kÃ¶tÃ¼: Ã‡ok fazla yanlÄ±ÅŸ alarm Ã¼retiyor")
    elif recall > 0.8:
        explanation.append(f"   âœ… Model kazanÃ§ fÄ±rsatlarÄ±nÄ± iyi yakalÄ±yor (%{recall*100:.1f})")
    else:
        explanation.append(f"   âš ï¸  Model Ã§ok fazla fÄ±rsat kaÃ§Ä±rÄ±yor (%{recall*100:.1f})")
    
    if balanced_acc < 0.5:
        explanation.append(f"   ğŸš¨ Ã–NEMLI: Balanced Accuracy (%{balanced_acc*100:.1f}) rastgele tahminden kÃ¶tÃ¼!")
        explanation.append(f"   ğŸ’¡ Model bir sÄ±nÄ±fa (muhtemelen 'OYNA') Ã§ok yanlÄ±")
    
    # DetaylÄ± sonuÃ§ analizi
    explanation.append(f"\nğŸ“‹ DETAYLI SONUÃ‡ ANALÄ°ZÄ°:")
    explanation.append(f"   âœ… DoÄŸru 'OYNA' tavsiyeleri: {int(tp)} adet")
    explanation.append(f"   âŒ YanlÄ±ÅŸ 'OYNA' tavsiyeleri: {int(fp)} adet (Para kaybÄ± riski!)")
    explanation.append(f"   âŒ KaÃ§Ä±rÄ±lan fÄ±rsatlar: {int(fn)} adet (Missed opportunities)")
    explanation.append(f"   âœ… DoÄŸru 'OYNAMA' tavsiyeleri: {int(tn)} adet")
    
    # Risk analizi
    if fp > 0 and tp > 0:
        risk_ratio = fp / (tp + fp)
        explanation.append(f"\nâš–ï¸  RÄ°SK ANALÄ°ZÄ°:")
        explanation.append(f"   â€¢ Model 'OYNA' dediÄŸinde %{(1-risk_ratio)*100:.1f} ihtimalle doÄŸru")
        explanation.append(f"   â€¢ Model 'OYNA' dediÄŸinde %{risk_ratio*100:.1f} ihtimalle yanlÄ±ÅŸ (RÄ°SK!)")
        
        if risk_ratio > 0.4:
            explanation.append(f"   ğŸš¨ UYARI: Ã‡ok yÃ¼ksek risk oranÄ±! CanlÄ± kullanÄ±m iÃ§in tehlikeli")
        elif risk_ratio > 0.2:
            explanation.append(f"   âš ï¸  Orta seviye risk. Dikkatli kullanÄ±m gerekli")
        else:
            explanation.append(f"   âœ… Kabul edilebilir risk seviyesi")
    
    # Trading perspektifi
    explanation.append(f"\nğŸ’° TRADÄ°NG PERSPEKTÄ°FÄ°:")
    if tn == 0:
        explanation.append(f"   ğŸš¨ KRÄ°TÄ°K: Model hiÃ§ 'OYNAMA' tavsiyesi vermiyor!")
        explanation.append(f"   ğŸ“Š Bu durumda model sadece agresif bir 'her zaman oyna' stratejisi")
        explanation.append(f"   âš ï¸  GerÃ§ek bir tahmin sistemi deÄŸil, dikkatli olun!")
    else:
        explanation.append(f"   âœ… Model hem 'OYNA' hem 'OYNAMA' tavsiyeleri veriyor")
    
    # MAE ve RMSE aÃ§Ä±klamasÄ±
    mae = metrics.get('mae', 0)
    rmse = metrics.get('rmse', 0)
    
    explanation.append(f"\nğŸ“ TAHMIN HATASI ANALÄ°ZÄ°:")
    explanation.append(f"   â€¢ Ortalama hata (MAE): {mae:.2f} birim")
    explanation.append(f"   â€¢ BÃ¼yÃ¼k hatalar (RMSE): {rmse:.2f} birim")
    
    if rmse > mae * 2:
        explanation.append(f"   âš ï¸  RMSE Ã§ok yÃ¼ksek: Model bazen Ã§ok bÃ¼yÃ¼k hatalar yapÄ±yor")
    else:
        explanation.append(f"   âœ… Hatalar genel olarak tutarlÄ±")
    
    # Ã–neri bÃ¶lÃ¼mÃ¼
    explanation.append(f"\nğŸ’¡ Ã–NERÄ°LER:")
    
    if balanced_acc < 0.5:
        explanation.append(f"   1. Model Ã§ok yanlÄ± - class balancing gerekli")
        explanation.append(f"   2. Threshold deÄŸerini ayarlayÄ±n (ÅŸu an 1.5)")
        explanation.append(f"   3. Loss function'Ä± false positive'leri cezalandÄ±racak ÅŸekilde dÃ¼zenleyin")
    
    if recall > 0.95 and precision < 0.8:
        explanation.append(f"   4. Model Ã§ok agresif - daha muhafazakar olmalÄ±")
        explanation.append(f"   5. Precision/Recall dengesini ayarlayÄ±n")
    
    if tn == 0:
        explanation.append(f"   6. ğŸš¨ EN Ã–NEMLÄ°SÄ°: Model 'OYNAMA' Ã¶ÄŸrenemiyor - ciddi problem!")
        explanation.append(f"   7. Veri dengesizliÄŸi var - negatif Ã¶rnekleri artÄ±rÄ±n")
    
    explanation.append(f"\nâ­ GENEL DEÄERLENDÄ°RME:")
    if f1 > 0.8 and balanced_acc > 0.6:
        explanation.append(f"   âœ… Model genel olarak iyi performans gÃ¶steriyor")
    elif f1 > 0.6 and balanced_acc > 0.5:
        explanation.append(f"   ğŸŸ¡ Model orta seviye - iyileÅŸtirilebilir")
    else:
        explanation.append(f"   ğŸ”´ Model performansÄ± yetersiz - ciddi revizyon gerekli")
        explanation.append(f"   ğŸ“ Ã–zellikle canlÄ± trading iÃ§in henÃ¼z kullanÄ±ma hazÄ±r deÄŸil")
    
    explanation.append("=" * 80)
    
    return "\n".join(explanation)

def generate_text_report(results: Dict[str, List[Dict[str, Any]]]) -> str:
    """
    Generates a text-based summary report from training results.

    Args:
        results: A dictionary containing results for each model.

    Returns:
        A formatted string with the summary report.
    """
    report_lines = []
    report_lines.append("=" * 60)
    report_lines.append("ğŸ“Š JETX MODEL TRAINING - FINAL PERFORMANCE REPORT")
    report_lines.append(f"Rapor Tarihi: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("=" * 60)

    if not results:
        report_lines.append("EÄŸitim sonucu bulunamadÄ±.")
        return "\n".join(report_lines)

    # Find the best overall model
    best_overall_model = None
    best_f1 = -1.0

    for model_name, model_results in results.items():
        if not model_results:
            continue
        
        # En iyi dÃ¶ngÃ¼yÃ¼ F1 skoruna gÃ¶re bul
        best_cycle = max(model_results, key=lambda x: x['performance'].get('f1', 0))
        if best_cycle['performance'].get('f1', 0) > best_f1:
            best_f1 = best_cycle['performance']['f1']
            best_overall_model = {
                'name': model_name,
                'f1': best_f1,
                'recall': best_cycle['performance'].get('recall', 0),
                'precision': best_cycle['performance'].get('precision', 0),
                'mae': best_cycle['performance'].get('mae', 0)
            }

    if best_overall_model:
        report_lines.append("\nğŸ† EN Ä°YÄ° MODEL (F1-Skoruna GÃ¶re)")
        report_lines.append(f"   - Model: {best_overall_model['name']}")
        report_lines.append(f"   - F1-Skoru: {best_overall_model['f1']:.4f}")
        report_lines.append(f"   - Recall (DuyarlÄ±lÄ±k): {best_overall_model['recall']:.4f} (Crash'leri yakalama oranÄ±)")
        report_lines.append(f"   - Precision (Kesinlik): {best_overall_model['precision']:.4f}")
        report_lines.append(f"   - Ortalama Mutlak Hata (MAE): {best_overall_model['mae']:.4f}")

    report_lines.append("\n" + "-" * 60)
    report_lines.append("MODEL DETAYLARI")
    report_lines.append("-" * 60)

    for model_name, model_results in results.items():
        if not model_results:
            report_lines.append(f"\n--- {model_name} ---")
            report_lines.append("  BaÅŸarÄ±lÄ± dÃ¶ngÃ¼ bulunamadÄ±.")
            continue

        best_cycle = max(model_results, key=lambda x: x['performance'].get('f1', 0))
        avg_f1 = np.mean([r['performance'].get('f1', 0) for r in model_results])
        avg_recall = np.mean([r['performance'].get('recall', 0) for r in model_results])
        avg_mae = np.mean([r['performance'].get('mae', 0) for r in model_results])

        report_lines.append(f"\n--- {model_name} ---")
        report_lines.append(f"  - Toplam DÃ¶ngÃ¼: {len(model_results)}")
        report_lines.append(f"  - Ortalama F1-Skoru: {avg_f1:.4f}")
        report_lines.append(f"  - Ortalama Recall: {avg_recall:.4f}")
        report_lines.append(f"  - Ortalama MAE: {avg_mae:.4f}")
        report_lines.append(f"  - En Ä°yi DÃ¶ngÃ¼ (Cycle {best_cycle['cycle']}):")
        report_lines.append(f"    - F1-Skoru: {best_cycle['performance'].get('f1', 0):.4f}")
        report_lines.append(f"    - Recall: {best_cycle['performance'].get('recall', 0):.4f}")
        report_lines.append(f"    - MAE: {best_cycle['performance'].get('mae', 0):.4f}")
        report_lines.append(f"    - Model Yolu: {os.path.basename(best_cycle['model_path'])}")

    return "\n".join(report_lines)

def generate_performance_plot(results: Dict[str, List[Dict[str, Any]]], save_path: str) -> bool:
    """
    Generates and saves a bar plot comparing model performances.

    Args:
        results: A dictionary containing results for each model.
        save_path: Path to save the plot image.

    Returns:
        True if the plot was saved successfully, False otherwise.
    """
    if not results:
        return False

    performance_data = []
    for model_name, model_results in results.items():
        if model_results:
            best_cycle = max(model_results, key=lambda x: x['performance'].get('f1', 0))
            performance_data.append({
                'Model': model_name,
                'MAE': best_cycle['performance'].get('mae', 0),
                'F1-Score': best_cycle['performance'].get('f1', 0),
                'Recall': best_cycle['performance'].get('recall', 0)
            })

    if not performance_data:
        return False

    df = pd.DataFrame(performance_data)
    
    plt.style.use('seaborn-v0_8-whitegrid')
    fig, axes = plt.subplots(2, 1, figsize=(12, 10))
    fig.suptitle('Model Performans KarÅŸÄ±laÅŸtÄ±rmasÄ± (En Ä°yi DÃ¶ngÃ¼ler)', fontsize=16, weight='bold')

    # MAE Plot (lower is better)
    sns.barplot(x='Model', y='MAE', data=df.sort_values('MAE', ascending=True), ax=axes[0], palette='viridis')
    axes[0].set_title('Ortalama Mutlak Hata (MAE) - DÃ¼ÅŸÃ¼k olan daha iyi')
    axes[0].set_ylabel('MAE')
    for container in axes[0].containers:
        axes[0].bar_label(container, fmt='%.4f')

    # F1-Score Plot (higher is better)
    sns.barplot(x='Model', y='F1-Score', data=df.sort_values('F1-Score', ascending=False), ax=axes[1], palette='plasma')
    axes[1].set_title('F1-Skoru - YÃ¼ksek olan daha iyi (Recall ve Precision Dengesi)')
    axes[1].set_ylabel('F1-Score')
    axes[1].set_ylim(0, max(1.0, df['F1-Score'].max() * 1.1))
    for container in axes[1].containers:
        axes[1].bar_label(container, fmt='%.4f')

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    
    try:
        plt.savefig(save_path)
        plt.close(fig)
        print(f"âœ… Performans grafiÄŸi kaydedildi: {save_path}")
        return True
    except Exception as e:
        print(f"âŒ Grafik kaydedilemedi: {e}")
        return False
