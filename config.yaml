# ===================================================================
# Merkezi Konfigürasyon Dosyası - JetX Tahmin Sistemi (v2)
# ===================================================================

# -------------------------------------------------------------------
# Veritabanı ve Dosya Yolları
# -------------------------------------------------------------------
paths:
  database: "data/jetx_data.db"
  models_dir: "trained_models"
  log_file: "jetx_prediction.log"
  cache_dir: "data/cache" # Yeni önbellek dizini

# -------------------------------------------------------------------
# Genel Eğitim ve HPO Ayarları (10 GB GPU Optimized)
# -------------------------------------------------------------------
training:
  hpo_trials: 25 # GPU optimized: 15 → 25 (more comprehensive search)
  default_epochs: 75 # GPU optimized: 50 → 75 (better convergence)
  batch_size: 64 # GPU optimized: 16 → 64 (4x increase for 10GB GPU)
  validation_split: 0.15
  min_data_points: 500
  threshold: 1.5
  model_sequence_length: 150 # GPU optimized: 50 → 150 (3x increase)
  feature_windows: [50, 100, 200, 500] # GPU optimized: expanded feature space
  lag_windows: [5, 10, 20, 50] # GPU optimized: restored full range
  lags: [1, 2, 3, 5, 10, 15] # GPU optimized: expanded lag features

# Memory management and SHAP optimization settings (12 GB GPU)
memory_optimization:
  enable_shap: true
  shap_frequency: 3 # Generate SHAP more frequently (every 3 cycles)
  shap_background_samples: 10 # Increased from 3 to 10 for better explanations
  shap_max_features: 20 # Show top 20 features instead of 10
  memory_limit_mb: 32000 # 32GB memory limit for 10GB GPU system
  cleanup_frequency: 5 # Cleanup memory every 5 cycles (less frequent)

# GPU Optimization Settings (12GB Memory Constraint)
gpu_optimization:
  max_memory_gb: 12.0 # Hard limit for GPU memory usage
  use_mixed_precision: true # FP16 training for 2x speed + 50% memory reduction
  gradient_accumulation_steps: 2 # Effective batch size multiplier without memory increase
  dynamic_batch_sizing: true # Automatically adjust batch size based on GPU memory
  memory_monitoring: true # Real-time GPU memory monitoring
  emergency_cleanup: true # Automatic cleanup when approaching memory limit
  target_utilization: 85 # Target GPU utilization percentage
  safety_margin_gb: 1.5 # Reserved memory for safety (avoid OOM)

# ===================================================================
# Model Yapılandırmaları (Merkezileştirilmiş)
# ===================================================================
models:
  N-Beats:
    default_params:
      hidden_size: 1024 # GPU optimized: 512 → 1024
      num_stacks: 6 # GPU optimized: 4 → 6
      num_blocks: 6 # GPU optimized: 4 → 6
      learning_rate: 0.0005
      crash_weight: 10.0  # CRITICAL FIX: 3.0 → 10.0 (Fix Precision=0 problem)
      train_params:
        epochs: 75
        batch_size: 64 # GPU optimized: 32 → 64
        validation_split: 0.15
        verbose: true
    hpo_space:
      learning_rate: { type: 'float', low: 0.00001, high: 0.01, log: false }
      hidden_size: { type: 'categorical', choices: [512, 1024, 2048, 4096] } # GPU optimized for 10GB
      num_stacks: { type: 'int', low: 4, high: 10 } # GPU optimized: expanded range
      num_blocks: { type: 'int', low: 4, high: 10 } # GPU optimized: expanded range

  TFT:
    default_params:
      hidden_size: 512 # GPU optimized: 256 → 512
      num_heads: 16 # GPU optimized: 8 → 16
      num_layers: 4 # GPU optimized: 3 → 4
      learning_rate: 0.0005
      crash_weight: 5.0
      train_params:
        epochs: 75 # GPU optimized: 60 → 75
        batch_size: 32 # GPU optimized: 16 → 32
        validation_split: 0.15
        verbose: true
    hpo_space:
      learning_rate: { type: 'float', low: 0.00001, high: 0.01, log: false }
      hidden_size: { type: 'categorical', choices: [256, 512, 1024, 2048] } # GPU optimized for 10GB
      num_heads: { type: 'categorical', choices: [8, 16, 32, 64] } # GPU optimized: expanded
      num_layers: { type: 'int', low: 3, high: 8 } # GPU optimized: expanded range

  LSTM:
    default_params:
      hidden_size: 512 # GPU optimized: 256 → 512
      num_layers: 4 # GPU optimized: 3 → 4
      learning_rate: 0.0005
      crash_weight: 5.0
      train_params:
        epochs: 100 # GPU optimized: 80 → 100
        batch_size: 64 # GPU optimized: 32 → 64
        validation_split: 0.15
        verbose: true
    hpo_space:
      learning_rate: { type: 'float', low: 0.00001, high: 0.01, log: false }
      hidden_size: { type: 'categorical', choices: [256, 512, 1024, 2048] } # GPU optimized for 10GB
      num_layers: { type: 'int', low: 3, high: 8 } # GPU optimized: expanded range

# ===================================================================
# Diğer Ayarlar
# ===================================================================
logging:
  level: 'INFO'
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
