{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Tam Otomatik JetX Model EÄŸitmeni (v3.0)\n",
    "\n",
    "Bu not defteri, JetX tahmin sistemi iÃ§in yeniden yapÄ±landÄ±rÄ±lmÄ±ÅŸ ve tam otomatik bir eÄŸitim akÄ±ÅŸÄ± saÄŸlar.\n",
    "\n",
    "## âœ¨ Yeni Ã–zellikler (v3.0)\n",
    "\n",
    "- **Otomatik Hiperparametre Optimizasyonu (HPO):** `Optuna` entegrasyonu ile her model iÃ§in en iyi hiperparametreler otomatik olarak bulunur.\n",
    "- **Deney Takibi:** `MLflow` entegrasyonu ile tÃ¼m eÄŸitim sÃ¼reÃ§leri, metrikler ve modeller kaydedilir ve izlenebilir.\n",
    "- **Merkezi YapÄ±landÄ±rma:** TÃ¼m ayarlar artÄ±k proje kÃ¶k dizinindeki `config.yaml` dosyasÄ±ndan yÃ¶netilir.\n",
    "- **Kod Standardizasyonu:** Proje tamamen `PyTorch` Ã¼zerine odaklanmÄ±ÅŸ, eski ve kullanÄ±lmayan kodlar temizlenmiÅŸtir.\n",
    "\n",
    "**âš ï¸ BaÅŸlamadan Ã–nce: Runtime > Change runtime type > GPU seÃ§eneÄŸini etkinleÅŸtirin!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ 1. Sistem Kurulumu ve Projeyi YÃ¼kleme"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "install_dependencies"
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"Clones or updates the repository and sets up the environment.\"\"\"\n",
    "    repo_url = \"https://github.com/onndd/predictor_1.git\"\n",
    "    project_dir = \"/content/predictor_1\"\n",
    "\n",
    "    if os.path.exists(project_dir):\n",
    "        print(\"ğŸ“ Proje zaten mevcut. GÃ¼ncelleniyor...\")\n",
    "        os.chdir(project_dir)\n",
    "        subprocess.run([\"git\", \"pull\", \"origin\", \"main\"], check=True, capture_output=True)\n",
    "    else:\n",
    "        print(\"ğŸ“¥ GitHub'dan proje klonlanÄ±yor...\")\n",
    "        os.chdir(\"/content\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True, capture_output=True)\n",
    "        os.chdir(project_dir)\n",
    "\n",
    "    # Add src to Python path\n",
    "    src_path = os.path.join(project_dir, \"src\")\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "        print(f\"âœ… {src_path} Python path'e eklendi\")\n",
    "\n",
    "    # Install dependencies\n",
    "    requirements_path = os.path.join(project_dir, 'requirements_enhanced.txt')\n",
    "    if os.path.exists(requirements_path):\n",
    "        print(\"ğŸ“¦ Gerekli kÃ¼tÃ¼phaneler yÃ¼kleniyor...\")\n",
    "        # --quiet bayraÄŸÄ± kaldÄ±rÄ±ldÄ±\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", requirements_path])\n",
    "        print(\"âœ… KÃ¼tÃ¼phaneler baÅŸarÄ±yla yÃ¼klendi.\")\n",
    "\n",
    "setup_environment()\n",
    "print(\"ğŸ‰ Kurulum tamamlandÄ±!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 2. MLflow Deney Takibini BaÅŸlatma\n",
    "\n",
    "Bu hÃ¼cre, MLflow arayÃ¼zÃ¼nÃ¼ baÅŸlatÄ±r ve `ngrok` kullanarak size eriÅŸebileceÄŸiniz bir public URL verir. EÄŸitim metriklerini ve sonuÃ§larÄ±nÄ± bu arayÃ¼zden takip edebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "start_mlflow"
   },
   "source": [
    "# ngrok kurulumu\n",
    "!pip install pyngrok --quiet\n",
    "\n",
    "# MLflow UI'Ä± arka planda baÅŸlat\n",
    "get_ipython().system_raw('mlflow ui --host 0.0.0.0 &')\n",
    "\n",
    "# ngrok ile public URL oluÅŸtur\n",
    "from pyngrok import ngrok\n",
    "print(\"âœ… MLflow arayÃ¼zÃ¼ iÃ§in public URL oluÅŸturuluyor...\")\n",
    "print(\"ğŸ‘‡ AÅŸaÄŸÄ±daki linke tÄ±klayarak deneyleri izleyebilirsiniz:\")\n",
    "print(ngrok.connect(5000))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 3. Otomatik EÄŸitim SÃ¼recini BaÅŸlatma"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "run_training"
   },
   "source": [
    "import torch\n",
    "from src.training.master_trainer import MasterTrainer\n",
    "import traceback\n",
    "\n",
    "# Cihaz tespiti (GPU veya CPU)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    print(f\"âœ… GPU bulundu: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"âš ï¸ GPU bulunamadÄ±. EÄŸitim CPU ile devam edecek.\")\n",
    "\n",
    "try:\n",
    "    # MasterTrainer'Ä± baÅŸlat ve tÃ¼m eÄŸitim sÃ¼recini Ã§alÄ±ÅŸtÄ±r.\n",
    "    # Bu iÅŸlem HPO, eÄŸitim ve MLflow kaydÄ±nÄ± iÃ§erir.\n",
    "    master_trainer = MasterTrainer(models_to_train=['N-Beats', 'TFT', 'LSTM'], device=device)\n",
    "    master_trainer.run()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ EÄŸitim sÄ±rasÄ±nda beklenmedik bir hata oluÅŸtu: {e}\")\n",
    "    traceback.print_exc()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
