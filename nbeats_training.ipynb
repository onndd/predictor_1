{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbeats_header"
      },
      "source": [
        "# ğŸ§  N-Beats Model EÄŸitimi - Google Colab\n",
        "\n",
        "Bu notebook N-Beats (Neural Basis Expansion Analysis for Time Series) modelini JetX verileri Ã¼zerinde eÄŸitir.\n",
        "\n",
        "## ğŸ“‹ N-Beats Ã–zellikleri:\n",
        "- **Interpretable**: Trend ve seasonality ayrÄ±ÅŸtÄ±rmasÄ±\n",
        "- **JetX Optimized**: Crash/pump pattern detection\n",
        "- **GPU Accelerated**: PyTorch CUDA desteÄŸi\n",
        "- **Multi-output**: Value, probability, confidence\n",
        "\n",
        "---\n",
        "\n",
        "**âš ï¸ Ã–nce `colab_setup.ipynb` notebook'unu Ã§alÄ±ÅŸtÄ±rÄ±n!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbeats_setup"
      },
      "outputs": [],
      "source": [
        "# ğŸ”§ Kurulum ve Import'lar\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Proje path'i ekle\n",
        "sys.path.insert(0, '/content/predictor_1/src')\n",
        "\n",
        "# Model import'larÄ±\n",
        "from models.deep_learning.n_beats.n_beats_model import NBeatsPredictor\n",
        "from data_processing.loader import load_data_from_sqlite\n",
        "from data_processing.splitter import create_sequences\n",
        "\n",
        "# GPU kontrolÃ¼\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ğŸ¯ Device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ“± GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "\n",
        "# Matplotlib iÃ§in stil\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… N-Beats kurulumu tamamlandÄ±!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "# ğŸ“Š Veri YÃ¼kleme\n",
        "print(\"ğŸ“Š JETX VERÄ°SÄ° YÃœKLEME\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Colab'dan veritabanÄ±nÄ± yÃ¼kle\n",
        "db_path = \"/content/colab_jetx_data.db\"\n",
        "df = load_data_from_sqlite(db_path)\n",
        "\n",
        "# Veriyi liste formatÄ±na Ã§evir\n",
        "jetx_data = [row[1] for row in df.data]  # value column\n",
        "print(f\"âœ… {len(jetx_data)} JetX verisi yÃ¼klendi\")\n",
        "\n",
        "# Veri analizi\n",
        "jetx_array = np.array(jetx_data)\n",
        "print(f\"ğŸ“ˆ Ä°statistikler:\")\n",
        "print(f\"   - Min: {jetx_array.min():.2f}\")\n",
        "print(f\"   - Max: {jetx_array.max():.2f}\")\n",
        "print(f\"   - Mean: {jetx_array.mean():.2f}\")\n",
        "print(f\"   - Std: {jetx_array.std():.2f}\")\n",
        "print(f\"   - Crash rate (<1.5): {(jetx_array < 1.5).mean()*100:.1f}%\")\n",
        "\n",
        "# Veri gÃ¶rselleÅŸtirmesi\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Time series plot\n",
        "axes[0,0].plot(jetx_data[:200])\n",
        "axes[0,0].set_title('Ä°lk 200 JetX DeÄŸeri')\n",
        "axes[0,0].set_xlabel('Zaman')\n",
        "axes[0,0].set_ylabel('JetX DeÄŸeri')\n",
        "axes[0,0].axhline(y=1.5, color='r', linestyle='--', alpha=0.7, label='Threshold')\n",
        "axes[0,0].legend()\n",
        "\n",
        "# Histogram\n",
        "axes[0,1].hist(jetx_data, bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[0,1].set_title('JetX DeÄŸer DaÄŸÄ±lÄ±mÄ±')\n",
        "axes[0,1].set_xlabel('JetX DeÄŸeri')\n",
        "axes[0,1].set_ylabel('Frekans')\n",
        "axes[0,1].axvline(x=1.5, color='r', linestyle='--', alpha=0.7, label='Threshold')\n",
        "axes[0,1].legend()\n",
        "\n",
        "# Box plot\n",
        "axes[1,0].boxplot(jetx_data)\n",
        "axes[1,0].set_title('JetX DeÄŸer Box Plot')\n",
        "axes[1,0].set_ylabel('JetX DeÄŸeri')\n",
        "\n",
        "# Autocorrelation\n",
        "lags = range(1, min(50, len(jetx_data)//10))\n",
        "autocorr = [np.corrcoef(jetx_data[:-lag], jetx_data[lag:])[0,1] for lag in lags]\n",
        "axes[1,1].plot(lags, autocorr)\n",
        "axes[1,1].set_title('Autocorrelation')\n",
        "axes[1,1].set_xlabel('Lag')\n",
        "axes[1,1].set_ylabel('Correlation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ğŸ¯ Veri analizi tamamlandÄ±!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_data"
      },
      "outputs": [],
      "source": [
        "# ğŸ”§ Veri HazÄ±rlama\n",
        "print(\"ğŸ”§ N-BEATS Ä°Ã‡Ä°N VERÄ° HAZIRLAMA\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# N-Beats iÃ§in parametreler\n",
        "SEQUENCE_LENGTH = 100  # Daha kÄ±sa sequence GPU memory iÃ§in\n",
        "TRAIN_SPLIT = 0.8\n",
        "VALIDATION_SPLIT = 0.1\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Veriyi train/val/test olarak bÃ¶l\n",
        "n_train = int(len(jetx_data) * TRAIN_SPLIT)\n",
        "n_val = int(len(jetx_data) * VALIDATION_SPLIT)\n",
        "\n",
        "train_data = jetx_data[:n_train]\n",
        "val_data = jetx_data[n_train:n_train + n_val]\n",
        "test_data = jetx_data[n_train + n_val:]\n",
        "\n",
        "print(f\"ğŸ“Š Veri bÃ¶lÃ¼mÃ¼:\")\n",
        "print(f\"   - Train: {len(train_data)} samples\")\n",
        "print(f\"   - Validation: {len(val_data)} samples\")\n",
        "print(f\"   - Test: {len(test_data)} samples\")\n",
        "\n",
        "# Normalizasyon (optional)\n",
        "train_mean = np.mean(train_data)\n",
        "train_std = np.std(train_data)\n",
        "print(f\"ğŸ“ Normalizasyon parametreleri:\")\n",
        "print(f\"   - Mean: {train_mean:.3f}\")\n",
        "print(f\"   - Std: {train_std:.3f}\")\n",
        "\n",
        "print(\"âœ… Veri hazÄ±rlandÄ±!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create_model"
      },
      "outputs": [],
      "source": [
        "# ğŸ§  N-Beats Model OluÅŸturma\n",
        "print(\"ğŸ§  N-BEATS MODEL OLUÅTURMA\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Model parametreleri\n",
        "model_params = {\n",
        "    'sequence_length': SEQUENCE_LENGTH,\n",
        "    'hidden_size': 256,  # GPU memory iÃ§in optimize\n",
        "    'num_stacks': 3,\n",
        "    'num_blocks': 2,  # Daha az block GPU memory iÃ§in\n",
        "    'learning_rate': 0.001,\n",
        "    'threshold': 1.5,\n",
        "    'crash_weight': 2.0\n",
        "}\n",
        "\n",
        "print(f\"ğŸ”§ Model parametreleri:\")\n",
        "for key, value in model_params.items():\n",
        "    print(f\"   - {key}: {value}\")\n",
        "\n",
        "# Model oluÅŸtur\n",
        "nbeats_model = NBeatsPredictor(**model_params)\n",
        "\n",
        "# GPU'ya taÅŸÄ±\n",
        "if torch.cuda.is_available():\n",
        "    nbeats_model.model = nbeats_model.model.cuda()\n",
        "    print(\"âœ… Model GPU'ya taÅŸÄ±ndÄ±\")\n",
        "\n",
        "# Model Ã¶zeti\n",
        "total_params = sum(p.numel() for p in nbeats_model.model.parameters())\n",
        "trainable_params = sum(p.numel() for p in nbeats_model.model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"ğŸ“Š Model istatistikleri:\")\n",
        "print(f\"   - Total parameters: {total_params:,}\")\n",
        "print(f\"   - Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   - Model size: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
        "\n",
        "print(\"âœ… N-Beats model hazÄ±r!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model"
      },
      "outputs": [],
      "source": [
        "# ğŸš€ Model EÄŸitimi\n",
        "print(\"ğŸš€ N-BEATS MODEL EÄÄ°TÄ°MÄ°\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# EÄŸitim parametreleri\n",
        "EPOCHS = 50  # Colab zaman limiti iÃ§in\n",
        "VERBOSE = True\n",
        "\n",
        "print(f\"ğŸ¯ EÄŸitim baÅŸlÄ±yor...\")\n",
        "print(f\"   - Epochs: {EPOCHS}\")\n",
        "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   - Device: {device}\")\n",
        "\n",
        "# EÄŸitim baÅŸlangÄ±Ã§ zamanÄ±\n",
        "start_time = time.time()\n",
        "\n",
        "# Model eÄŸitimi\n",
        "try:\n",
        "    history = nbeats_model.train(\n",
        "        data=train_data,\n",
        "        epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_split=0.2,\n",
        "        verbose=VERBOSE\n",
        "    )\n",
        "    \n",
        "    # EÄŸitim sÃ¼resi\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nâ±ï¸ EÄŸitim sÃ¼resi: {training_time/60:.2f} dakika\")\n",
        "    \n",
        "    # EÄŸitim sonuÃ§larÄ±\n",
        "    final_train_loss = history['train_losses'][-1]\n",
        "    final_val_loss = history['val_losses'][-1]\n",
        "    \n",
        "    print(f\"ğŸ“Š Final sonuÃ§lar:\")\n",
        "    print(f\"   - Train Loss: {final_train_loss:.6f}\")\n",
        "    print(f\"   - Val Loss: {final_val_loss:.6f}\")\n",
        "    print(f\"   - Epochs completed: {len(history['train_losses'])}\")\n",
        "    \n",
        "    print(\"âœ… Model eÄŸitimi tamamlandÄ±!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ EÄŸitim hatasÄ±: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_training"
      },
      "outputs": [],
      "source": [
        "# ğŸ“ˆ EÄŸitim SonuÃ§larÄ± GÃ¶rselleÅŸtirme\n",
        "if 'history' in locals() and history:\n",
        "    print(\"ğŸ“ˆ EÄÄ°TÄ°M SONUÃ‡LARI GÃ–RSELLEÅTÄ°RME\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss curves\n",
        "    epochs_range = range(1, len(history['train_losses']) + 1)\n",
        "    \n",
        "    axes[0,0].plot(epochs_range, history['train_losses'], 'b-', label='Training Loss')\n",
        "    axes[0,0].plot(epochs_range, history['val_losses'], 'r-', label='Validation Loss')\n",
        "    axes[0,0].set_title('Model Loss')\n",
        "    axes[0,0].set_xlabel('Epochs')\n",
        "    axes[0,0].set_ylabel('Loss')\n",
        "    axes[0,0].legend()\n",
        "    axes[0,0].grid(True)\n",
        "    \n",
        "    # Loss zoom (son %20)\n",
        "    start_idx = max(0, int(len(history['train_losses']) * 0.8))\n",
        "    axes[0,1].plot(epochs_range[start_idx:], history['train_losses'][start_idx:], 'b-', label='Training Loss')\n",
        "    axes[0,1].plot(epochs_range[start_idx:], history['val_losses'][start_idx:], 'r-', label='Validation Loss')\n",
        "    axes[0,1].set_title('Model Loss (Son %20)')\n",
        "    axes[0,1].set_xlabel('Epochs')\n",
        "    axes[0,1].set_ylabel('Loss')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True)\n",
        "    \n",
        "    # Learning rate curve (eÄŸer var ise)\n",
        "    axes[1,0].plot(epochs_range, history['train_losses'], 'g-')\n",
        "    axes[1,0].set_title('Training Loss Trend')\n",
        "    axes[1,0].set_xlabel('Epochs')\n",
        "    axes[1,0].set_ylabel('Loss')\n",
        "    axes[1,0].grid(True)\n",
        "    \n",
        "    # Loss statistics\n",
        "    loss_stats = {\n",
        "        'Min Train Loss': min(history['train_losses']),\n",
        "        'Min Val Loss': min(history['val_losses']),\n",
        "        'Final Train Loss': history['train_losses'][-1],\n",
        "        'Final Val Loss': history['val_losses'][-1]\n",
        "    }\n",
        "    \n",
        "    axes[1,1].axis('off')\n",
        "    table_data = [[k, f\"{v:.6f}\"] for k, v in loss_stats.items()]\n",
        "    table = axes[1,1].table(cellText=table_data, colLabels=['Metric', 'Value'], loc='center')\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(10)\n",
        "    table.scale(1.2, 1.5)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"âœ… EÄŸitim sonuÃ§larÄ± gÃ¶rselleÅŸtirildi!\")\n",
        "else:\n",
        "    print(\"âš ï¸ EÄŸitim history bulunamadÄ±.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_model"
      },
      "outputs": [],
      "source": [
        "# ğŸ§ª Model Test ve DeÄŸerlendirme\n",
        "print(\"ğŸ§ª MODEL TEST VE DEÄERLENDÄ°RME\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test verisi Ã¼zerinde tahminler\n",
        "test_predictions = []\n",
        "test_actuals = []\n",
        "test_probabilities = []\n",
        "test_confidences = []\n",
        "\n",
        "print(\"ğŸ” Test verisi Ã¼zerinde tahminler yapÄ±lÄ±yor...\")\n",
        "\n",
        "for i in tqdm(range(len(test_data) - SEQUENCE_LENGTH)):\n",
        "    # Sequence hazÄ±rla\n",
        "    sequence = test_data[i:i + SEQUENCE_LENGTH]\n",
        "    actual = test_data[i + SEQUENCE_LENGTH]\n",
        "    \n",
        "    try:\n",
        "        # Tahmin yap\n",
        "        predicted_value, probability, confidence = nbeats_model.predict(sequence)\n",
        "        \n",
        "        test_predictions.append(predicted_value)\n",
        "        test_actuals.append(actual)\n",
        "        test_probabilities.append(probability)\n",
        "        test_confidences.append(confidence)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ Tahmin hatasÄ±: {e}\")\n",
        "        break\n",
        "\n",
        "print(f\"âœ… {len(test_predictions)} tahmin yapÄ±ldÄ±\")\n",
        "\n",
        "# Metrikleri hesapla\n",
        "if test_predictions:\n",
        "    test_predictions = np.array(test_predictions)\n",
        "    test_actuals = np.array(test_actuals)\n",
        "    test_probabilities = np.array(test_probabilities)\n",
        "    test_confidences = np.array(test_confidences)\n",
        "    \n",
        "    # Regression metrikleri\n",
        "    mae = np.mean(np.abs(test_predictions - test_actuals))\n",
        "    mse = np.mean((test_predictions - test_actuals) ** 2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    \n",
        "    # Classification metrikleri (threshold = 1.5)\n",
        "    actual_binary = (test_actuals >= 1.5).astype(int)\n",
        "    pred_binary = (test_probabilities >= 0.5).astype(int)\n",
        "    \n",
        "    accuracy = np.mean(actual_binary == pred_binary)\n",
        "    precision = np.sum((pred_binary == 1) & (actual_binary == 1)) / np.sum(pred_binary == 1) if np.sum(pred_binary == 1) > 0 else 0\n",
        "    recall = np.sum((pred_binary == 1) & (actual_binary == 1)) / np.sum(actual_binary == 1) if np.sum(actual_binary == 1) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    print(f\"ğŸ“Š TEST SONUÃ‡LARI:\")\n",
        "    print(f\"   Regression Metrics:\")\n",
        "    print(f\"   - MAE: {mae:.4f}\")\n",
        "    print(f\"   - RMSE: {rmse:.4f}\")\n",
        "    print(f\"   Classification Metrics:\")\n",
        "    print(f\"   - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"   - Precision: {precision:.4f}\")\n",
        "    print(f\"   - Recall: {recall:.4f}\")\n",
        "    print(f\"   - F1-Score: {f1:.4f}\")\n",
        "    print(f\"   Confidence:\")\n",
        "    print(f\"   - Mean Confidence: {np.mean(test_confidences):.4f}\")\n",
        "    print(f\"   - Std Confidence: {np.std(test_confidences):.4f}\")\n",
        "\n",
        "print(\"âœ… Model deÄŸerlendirmesi tamamlandÄ±!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot_predictions"
      },
      "outputs": [],
      "source": [
        "# ğŸ“Š Tahmin SonuÃ§larÄ± GÃ¶rselleÅŸtirme\n",
        "if 'test_predictions' in locals() and len(test_predictions) > 0:\n",
        "    print(\"ğŸ“Š TAHMÄ°N SONUÃ‡LARI GÃ–RSELLEÅTÄ°RME\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Actual vs Predicted\n",
        "    axes[0,0].scatter(test_actuals, test_predictions, alpha=0.6)\n",
        "    axes[0,0].plot([test_actuals.min(), test_actuals.max()], [test_actuals.min(), test_actuals.max()], 'r--', alpha=0.8)\n",
        "    axes[0,0].set_xlabel('Actual Values')\n",
        "    axes[0,0].set_ylabel('Predicted Values')\n",
        "    axes[0,0].set_title('Actual vs Predicted')\n",
        "    axes[0,0].grid(True)\n",
        "    \n",
        "    # Time series comparison (ilk 100 tahmin)\n",
        "    n_show = min(100, len(test_predictions))\n",
        "    x_axis = range(n_show)\n",
        "    axes[0,1].plot(x_axis, test_actuals[:n_show], 'b-', label='Actual', alpha=0.7)\n",
        "    axes[0,1].plot(x_axis, test_predictions[:n_show], 'r-', label='Predicted', alpha=0.7)\n",
        "    axes[0,1].axhline(y=1.5, color='g', linestyle='--', alpha=0.5, label='Threshold')\n",
        "    axes[0,1].set_xlabel('Time Steps')\n",
        "    axes[0,1].set_ylabel('JetX Value')\n",
        "    axes[0,1].set_title(f'Time Series Comparison (Ä°lk {n_show} tahmin)')\n",
        "    axes[0,1].legend()\n",
        "    axes[0,1].grid(True)\n",
        "    \n",
        "    # Error distribution\n",
        "    errors = test_predictions - test_actuals\n",
        "    axes[1,0].hist(errors, bins=30, alpha=0.7, edgecolor='black')\n",
        "    axes[1,0].axvline(x=0, color='r', linestyle='--', alpha=0.7)\n",
        "    axes[1,0].set_xlabel('Prediction Error')\n",
        "    axes[1,0].set_ylabel('Frequency')\n",
        "    axes[1,0].set_title('Error Distribution')\n",
        "    axes[1,0].grid(True)\n",
        "    \n",
        "    # Confidence vs Error\n",
        "    axes[1,1].scatter(test_confidences, np.abs(errors), alpha=0.6)\n",
        "    axes[1,1].set_xlabel('Confidence')\n",
        "    axes[1,1].set_ylabel('Absolute Error')\n",
        "    axes[1,1].set_title('Confidence vs Absolute Error')\n",
        "    axes[1,1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"âœ… Tahmin sonuÃ§larÄ± gÃ¶rselleÅŸtirildi!\")\n",
        "else:\n",
        "    print(\"âš ï¸ Tahmin sonuÃ§larÄ± bulunamadÄ±.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_model"
      },
      "outputs": [],
      "source": [
        "# ğŸ’¾ Model Kaydetme\n",
        "print(\"ğŸ’¾ MODEL KAYDETME\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Model kaydetme dizini\n",
        "model_dir = \"/content/trained_models/\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Model dosya adÄ±\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "model_filename = f\"nbeats_model_{timestamp}.pth\"\n",
        "model_path = os.path.join(model_dir, model_filename)\n",
        "\n",
        "# Model kaydet\n",
        "try:\n",
        "    nbeats_model.save_model(model_path)\n",
        "    print(f\"âœ… Model kaydedildi: {model_path}\")\n",
        "    \n",
        "    # Model bilgilerini kaydet\n",
        "    model_info = {\n",
        "        'model_type': 'N-Beats',\n",
        "        'timestamp': timestamp,\n",
        "        'parameters': model_params,\n",
        "        'training_time': f\"{training_time/60:.2f} minutes\" if 'training_time' in locals() else 'Unknown',\n",
        "        'final_losses': {\n",
        "            'train': history['train_losses'][-1] if 'history' in locals() else 'Unknown',\n",
        "            'val': history['val_losses'][-1] if 'history' in locals() else 'Unknown'\n",
        "        },\n",
        "        'test_metrics': {\n",
        "            'mae': mae if 'mae' in locals() else 'Unknown',\n",
        "            'rmse': rmse if 'rmse' in locals() else 'Unknown',\n",
        "            'accuracy': accuracy if 'accuracy' in locals() else 'Unknown',\n",
        "            'f1_score': f1 if 'f1' in locals() else 'Unknown'\n",
        "        },\n",
        "        'data_info': {\n",
        "            'sequence_length': SEQUENCE_LENGTH,\n",
        "            'train_size': len(train_data),\n",
        "            'val_size': len(val_data),\n",
        "            'test_size': len(test_data)\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Model bilgilerini JSON olarak kaydet\n",
        "    import json\n",
        "    info_filename = f\"nbeats_info_{timestamp}.json\"\n",
        "    info_path = os.path.join(model_dir, info_filename)\n",
        "    \n",
        "    with open(info_path, 'w') as f:\n",
        "        json.dump(model_info, f, indent=2)\n",
        "    \n",
        "    print(f\"âœ… Model bilgileri kaydedildi: {info_path}\")\n",
        "    \n",
        "    # Google Drive'a kopyala (optional)\n",
        "    print(\"\\nğŸ’¡ Google Drive'a kaydetmek iÃ§in:\")\n",
        "    print(f\"   !cp {model_path} /content/drive/MyDrive/\")\n",
        "    print(f\"   !cp {info_path} /content/drive/MyDrive/\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"âŒ Model kaydetme hatasÄ±: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "print(\"\\nğŸ‰ N-Beats model eÄŸitimi tamamlandÄ±!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbeats_summary"
      },
      "source": [
        "# ğŸ¯ N-Beats EÄŸitimi Ã–zeti\n",
        "\n",
        "## âœ… Tamamlanan Ä°ÅŸlemler:\n",
        "1. **Veri HazÄ±rlama** - JetX verisi yÃ¼klendi ve iÅŸlendi\n",
        "2. **Model OluÅŸturma** - N-Beats modeli GPU'da oluÅŸturuldu\n",
        "3. **EÄŸitim** - Model JetX verisi Ã¼zerinde eÄŸitildi\n",
        "4. **DeÄŸerlendirme** - Test verisi Ã¼zerinde performans Ã¶lÃ§Ã¼ldÃ¼\n",
        "5. **GÃ¶rselleÅŸtirme** - EÄŸitim ve test sonuÃ§larÄ± gÃ¶rselleÅŸtirildi\n",
        "6. **Kaydetme** - Model ve bilgiler kaydedildi\n",
        "\n",
        "## ğŸ“Š Model Ã–zellikleri:\n",
        "- **Architecture**: Neural Basis Expansion Analysis\n",
        "- **Input**: JetX deÄŸer dizileri\n",
        "- **Output**: DeÄŸer, olasÄ±lÄ±k, gÃ¼ven skoru\n",
        "- **Optimization**: JetX crash/pump pattern detection\n",
        "- **Device**: GPU accelerated\n",
        "\n",
        "## ğŸš€ Sonraki AdÄ±mlar:\n",
        "1. **DiÄŸer Modeller** - TFT, LSTM, Ensemble eÄŸitimi\n",
        "2. **Hyperparameter Tuning** - Optuna ile optimizasyon\n",
        "3. **Production Deployment** - Model serving\n",
        "4. **Real-time Testing** - CanlÄ± veri testi\n",
        "\n",
        "---\n",
        "\n",
        "**ğŸ‰ N-Beats eÄŸitimi baÅŸarÄ±yla tamamlandÄ±!**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
